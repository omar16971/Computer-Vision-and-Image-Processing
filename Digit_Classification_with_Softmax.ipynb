{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61860dd0-f76e-44ac-b577-945ab5df6698",
   "metadata": {},
   "source": [
    "<h1>Digit Classification with Softmax</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c087a-f393-4c46-bd9e-f253adae5041",
   "metadata": {},
   "source": [
    "<h2>Objectives</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>Download the Training and Validation MNIST Digit Images</li>\n",
    "    <li>Create a Softmax Classifier using PyTorch</li>\n",
    "    <li>Create a Criterion, Optimizer, and Data Loaders</li>\n",
    "    <li>Create a Data Loader and set the Batch Size</li>\n",
    "    <li>Train a Model</li>\n",
    "    <li>Analyze Results and Model</li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffcb85e-71ef-47d0-86c4-ed69a6dd41ca",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7014dbc2-113c-4c32-8b89-35b8c6af9d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: torchvision in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: torchaudio in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/omaraffifi/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# install libraries\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bed7923d-3638-47fc-9447-77f56454f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# PyTorch Library\n",
    "import torch \n",
    "# PyTorch Neural Network\n",
    "import torch.nn as nn\n",
    "# Allows us to transform data\n",
    "import torchvision.transforms as transforms\n",
    "# Allows us to get the digit dataset\n",
    "import torchvision.datasets as dsets\n",
    "# Creating graphs\n",
    "import matplotlib.pylab as plt\n",
    "# Allows us to use arrays to manipulate and store data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fc4c407-2f27-443a-b13a-c53ed5d5e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the parameters\n",
    "\n",
    "def PlotParameters(model): \n",
    "    W = model.state_dict()['linear.weight'].data\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(2, 5)\n",
    "    fig.subplots_adjust(hspace=0.01, wspace=0.1)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < 10:\n",
    "            \n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"class: {0}\".format(i))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :].view(28, 28), vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        # Ensure the plot is shown correctly with multiple plots\n",
    "        # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7f27787-b2c6-49ea-b3cb-eaa9106b5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda56c0-2f28-4fbf-a226-7cdce927c5a9",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9737f7a-fd4f-4736-87d5-55b1fa9fd594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the training dataset:\n",
      "  Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# Create data and load the training data set\n",
    "train_dataset = dsets.MNIST(root = './data', train = True, download = True, transform = transforms.ToTensor())\n",
    "print(\"Print the training dataset:\\n \", train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f2e7cf3-2779-4ae2-b2a0-2e5a9c8739a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the validation dataset:\n",
      "  Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# Create data and load the validation data set\n",
    "validation_dataset = dsets.MNIST(root='./data', download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the validation dataset:\\n \", validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43583c-0bd5-4542-8993-5f5699f40682",
   "metadata": {},
   "source": [
    "### Accessing the data by indexing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "349f06be-d0a2-4442-8ee0-ecf85048b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label:  1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe label: \u001b[39m\u001b[38;5;124m\"\u001b[39m, train_dataset[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the image and label\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst Image and Label\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mshow_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m, in \u001b[0;36mshow_data\u001b[0;34m(data_sample)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_data\u001b[39m(data_sample):\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(data_sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my = \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mdata_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print the label\n",
    "\n",
    "print(\"The label: \", train_dataset[3][1])\n",
    "# Print the image and label\n",
    "print(\"First Image and Label\", show_data(train_dataset[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37e1a2-60f5-402f-933b-949af46514f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the label\n",
    "\n",
    "print(\"The label: \", train_dataset[0][1])\n",
    "# Plot the image\n",
    "\n",
    "print(\"The image: \", show_data(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bb8b4-b691-493d-ba59-cbe07f7537cc",
   "metadata": {},
   "source": [
    "## Build a SoftMax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95cf2f-7d8e-4d7c-84ff-9c739ecf6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax classifier class\n",
    "# Inherits nn.Module which is the base class for all neural networks\n",
    "class SoftMax(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SoftMax, self).__init__()\n",
    "        # Creates a layer of given input size and output size\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        # Runs the x value through the single layers defined above\n",
    "        z = self.linear(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa48cd-f7b7-4f09-88ee-737a9e479372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the training dataset\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c3b0b-0b33-484d-ae44-080b1f1f4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dimentions\n",
    "input_dim = 28 * 28\n",
    "# there are 10 possible digits\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed057e-d1f7-4bb7-82fb-d4349369dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftMax(input_dim, output_dim)\n",
    "print('Print the model:\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d60a6d-4f3a-4c95-b03e-11caf0bd404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the weight and bias parameters\n",
    "print('W: ', list(model.parameters())[0].size())\n",
    "print('B: ', list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8b43a-e27c-4ddb-b163-6648e33e500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model parameter before training\n",
    "PlotParameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f788c-11bf-4eef-99f4-7711b666f692",
   "metadata": {},
   "source": [
    " Before training the model, we have to flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768181ac-0df8-46c7-8ab6-e3415422e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening the x values\n",
    "X = train_dataset[0][0]\n",
    "X = X.view(-1, 28*28)\n",
    "print(X.shape)\n",
    "# Now we can make a prediction, each class has a value, and the higher it is the more confident the model is that it is that digit\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361a52e-5d9e-48f3-956b-7adbeeed6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate, optimizer, criterion, and data loader\n",
    "\n",
    "learning_rate = 0.1\n",
    "# the optimizer updates the model parameter as per the learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# The criterion will measure the loss between the prediction and actual label values\n",
    "# This is where the SoftMax occurs, it is built into the Criterion Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Created a training data loader so we can set the batch size\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 100)\n",
    "# Created a validation data loader so we can set the batch size\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdd600-c3ff-4ac0-9f85-19b3d9c60ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(X)\n",
    "actual = torch.tensor([train_dataset[0][1]])\n",
    "\n",
    "print(\"Output: \", model_output)\n",
    "print(\"Actual:\", actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1dcdc-904e-47d2-8f79-6315be3464b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(model_output, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba18d95-b6c3-47ba-bd72-c7b9eae3976f",
   "metadata": {},
   "source": [
    "cross entropy loss takes in probabilities, so we have to change the mode_output to a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c8803-a9d8-4560-b519-ffb8ea0b41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax changes the mode_output to a probability\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "probability = softmax(model_output)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f85c84-4f0c-41fe-8260-96c12f41ff52",
   "metadata": {},
   "source": [
    "Now that we have probabilities, we can just calculate the negative log of the probability of the class that this image belongs to. The image belongs to the target class so we calculate the negative log of the probability at the target index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7571e2-d9ff-4a8c-868e-659186aea890",
   "metadata": {},
   "outputs": [],
   "source": [
    "-1*torch.log(probability[0][actual])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123be44d-ea44-4cac-b190-a5f15d479b85",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb0f13-1e32-4566-8604-be992d19b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we train our model useing the training data\n",
    "n_epochs = 10\n",
    "# Lists to keep track of loss and accuracy\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "# Size of the validation data\n",
    "N_test = len(validation_dataset)\n",
    "\n",
    "# Function to train the model based on number of epochs\n",
    "def train_model(n_epochs):\n",
    "    # Loops n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        # For each batch in the train loader\n",
    "        for x, y in train_loader:\n",
    "            # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
    "            optimizer.zero_grad()\n",
    "            # Makes a prediction based on the image tensor\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            # Calculates loss between the model output and actual class\n",
    "            loss = criterion(z, y)\n",
    "            # Calculates the gradient value with respect to each weight and bias\n",
    "            loss.backward()\n",
    "            # Updates the weight and bias according to calculated gradient value\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Each epoch we check how the model performs with data it has not seen which is the validation data, we are not training here\n",
    "        correct = 0\n",
    "        # For each batch in the validation loader\n",
    "        for x_test, y_test in validation_loader:\n",
    "            # Makes prediction based on image tensor\n",
    "            z = model(x_test.view(-1, 28 * 28))\n",
    "            # Finds the class with the higest output\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            # Checks if the prediction matches the actual class and increments correct if it does\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        # Calculates the accuracy by dividing correct by size of validation dataset\n",
    "        accuracy = correct / N_test\n",
    "        # Keeps track loss\n",
    "        loss_list.append(loss.data)\n",
    "        # Keeps track of the accuracy\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "# Function call\n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211ca78-e739-406b-a6ba-75d7f14ed12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(loss_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b3ba6-7a7d-4477-ba91-0b4623acb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameters\n",
    "\n",
    "PlotParameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50109af5-0f37-4b53-ab9d-41f047f77f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the misclassified samples\n",
    "Softmax_fn=nn.Softmax(dim=-1)\n",
    "count = 0\n",
    "for x, y in validation_dataset:\n",
    "    z = model(x.reshape(-1, 28 * 28))\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    if yhat != y:\n",
    "        show_data((x, y))\n",
    "        plt.show()\n",
    "        print(\"yhat:\", yhat)\n",
    "        print(\"probability of class \", torch.max(Softmax_fn(z)).item())\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f5fa2-c1f9-4498-a455-5b909c65acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the classified samples\n",
    "Softmax_fn=nn.Softmax(dim=-1)\n",
    "count = 0\n",
    "for x, y in validation_dataset:\n",
    "    z = model(x.reshape(-1, 28 * 28))\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    if yhat == y:\n",
    "        show_data((x, y))\n",
    "        plt.show()\n",
    "        print(\"yhat:\", yhat)\n",
    "        print(\"probability of class \", torch.max(Softmax_fn(z)).item())\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187c770-5402-44ce-b446-2c8152e5b9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
